{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Extract, Transform and Load using Apache Spark SQL", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The notebook demonstrates how to use Apache Spark to ETL weather observation data stored in IBM Cloud Object Storage. The observation data generated on an hourly basis will be aggregated to show minimum, average and maximum night and day temperatures per day. The aggregarted information will finally be stored in a PostgreSQL database.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Extracting Weather Observation Data ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This section reads an IBM Cloud Object Storage JSON file that contains weather observation data from the Weather Data Company. The information will bve stored in a Spark data frame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190520072501-0000\nKERNEL_ID = 5a3878a5-6ceb-47d7-8c83-bbcb269a0ae6\n"
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "df = spark.read.json(path_1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "df.printSchema()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- _corrupt_record: string (nullable = true)\n |-- blunt_phrase: string (nullable = true)\n |-- class: string (nullable = true)\n |-- clds: string (nullable = true)\n |-- day_ind: string (nullable = true)\n |-- dewPt: long (nullable = true)\n |-- expire_time_gmt: long (nullable = true)\n |-- feels_like: long (nullable = true)\n |-- gust: long (nullable = true)\n |-- heat_index: long (nullable = true)\n |-- icon_extd: long (nullable = true)\n |-- key: string (nullable = true)\n |-- max_temp: string (nullable = true)\n |-- min_temp: string (nullable = true)\n |-- obs_id: string (nullable = true)\n |-- obs_name: string (nullable = true)\n |-- precip_hrly: string (nullable = true)\n |-- precip_total: string (nullable = true)\n |-- pressure: double (nullable = true)\n |-- pressure_desc: string (nullable = true)\n |-- pressure_tend: string (nullable = true)\n |-- primary_swell_direction: string (nullable = true)\n |-- primary_swell_height: string (nullable = true)\n |-- primary_swell_period: string (nullable = true)\n |-- primary_wave_height: string (nullable = true)\n |-- primary_wave_period: string (nullable = true)\n |-- qualifier: string (nullable = true)\n |-- qualifier_svrty: string (nullable = true)\n |-- rh: long (nullable = true)\n |-- secondary_swell_direction: string (nullable = true)\n |-- secondary_swell_height: string (nullable = true)\n |-- secondary_swell_period: string (nullable = true)\n |-- snow_hrly: string (nullable = true)\n |-- temp: long (nullable = true)\n |-- terse_phrase: string (nullable = true)\n |-- uv_desc: string (nullable = true)\n |-- uv_index: long (nullable = true)\n |-- valid_time_gmt: long (nullable = true)\n |-- vis: double (nullable = true)\n |-- water_temp: string (nullable = true)\n |-- wc: long (nullable = true)\n |-- wdir: long (nullable = true)\n |-- wdir_cardinal: string (nullable = true)\n |-- wspd: long (nullable = true)\n |-- wx_icon: long (nullable = true)\n |-- wx_phrase: string (nullable = true)\n\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "## Transforming Weather Observation Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this section the Spark data frame will be transformed by aggregating temperatures on a daily basis to show the minimum, average and maximum day and night temperature for for the locations. For this purpose a Spark SQL query will be used. A new data frame will then be created that represents the aggregated information in a relational format.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df.createOrReplaceTempView(\"observations\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "query = \"WITH wd AS (SELECT day_ind, from_unixtime(valid_time_gmt,'YYYY-MM-dd') as obs_date, obs_name, temp FROM observations WHERE class = 'observation') \\n \" + \\\n        \"SELECT wd.obs_name, wd.obs_date, wd.day_ind, ROUND(MIN(wd.temp),1) AS mintemp, ROUND(AVG(wd.temp),1) AS avgtemp, ROUND(MAX(wd.temp),1) AS maxtemp \\n \" + \\\n        \"FROM wd \\n \" + \\\n        \"GROUP BY wd.obs_name, wd.obs_date, wd.day_ind \\n \" + \\\n        \"ORDER BY wd.obs_name, wd.obs_date, wd.day_ind\"\n         \nprint(query)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "WITH wd AS (SELECT day_ind, from_unixtime(valid_time_gmt,'YYYY-MM-dd') as obs_date, obs_name, temp FROM observations WHERE class = 'observation') \n SELECT wd.obs_name, wd.obs_date, wd.day_ind, ROUND(MIN(wd.temp),1) AS mintemp, ROUND(AVG(wd.temp),1) AS avgtemp, ROUND(MAX(wd.temp),1) AS maxtemp \n FROM wd \n GROUP BY wd.obs_name, wd.obs_date, wd.day_ind \n ORDER BY wd.obs_name, wd.obs_date, wd.day_ind\n"
                }
            ], 
            "execution_count": 5
        }, 
        {
            "source": "sqlDF = spark.sql(query)\nsqlDF.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------------+----------+-------+-------+-------+-------+\n|            obs_name|  obs_date|day_ind|mintemp|avgtemp|maxtemp|\n+--------------------+----------+-------+-------+-------+-------+\n|Hamburg/Finkenwerder|2019-04-28|      D|      8|    8.9|     13|\n|Hamburg/Finkenwerder|2019-04-28|      N|      8|    9.0|     10|\n|Hamburg/Finkenwerder|2019-04-29|      D|      8|   11.7|     17|\n|Hamburg/Finkenwerder|2019-04-29|      N|      4|    8.0|     13|\n|Hamburg/Finkenwerder|2019-04-30|      D|      4|   11.9|     17|\n|Hamburg/Finkenwerder|2019-04-30|      N|      4|    8.0|     11|\n|Hamburg/Finkenwerder|2019-05-01|      D|      9|    9.6|     11|\n|Hamburg/Finkenwerder|2019-05-01|      N|      8|    8.6|      9|\n|Hamburg/Finkenwerder|2019-05-02|      D|      8|    9.9|     13|\n|Hamburg/Finkenwerder|2019-05-02|      N|      6|    7.4|      8|\n|Hamburg/Finkenwerder|2019-05-03|      D|      4|    7.5|      9|\n|Hamburg/Finkenwerder|2019-05-03|      N|      3|    4.1|      5|\n|Hamburg/Finkenwerder|2019-05-04|      D|      3|    6.7|     10|\n|Hamburg/Finkenwerder|2019-05-04|      N|      1|    2.4|      4|\n|Hamburg/Finkenwerder|2019-05-05|      D|     -1|    7.4|     11|\n|Hamburg/Finkenwerder|2019-05-05|      N|      0|    3.4|      7|\n|Hamburg/Finkenwerder|2019-05-06|      D|      3|    7.6|     10|\n|Hamburg/Finkenwerder|2019-05-06|      N|      2|    3.9|      6|\n|Hamburg/Finkenwerder|2019-05-07|      D|      2|    8.8|     12|\n|Hamburg/Finkenwerder|2019-05-07|      N|      1|    4.5|      7|\n+--------------------+----------+-------+-------+-------+-------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "## Loading Weather Observation Data to a PostgreSQL Database", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this section the aggregated data will be stored in a PostgreSQL table using the SQLAlchemy API.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import sqlalchemy\nimport pandas as pd\nsqlalchemy.__version__ ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "'1.1.13'"
                    }, 
                    "execution_count": 10, 
                    "metadata": {}
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "postgreSQLEndpoint = 'postgres://<username>:<password>@9a0a10ef-e7b8-402f-a80f-7d5d4e716622.bc28ac43cf10402584b5f01db462d330.databases.appdomain.cloud:30476/ibmclouddb'#?sslmode=verify-full'", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 8
        }, 
        {
            "source": "engine = sqlalchemy.create_engine(postgreSQLEndpoint)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/ibm/conda/miniconda3/lib/python3.5/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n  \"\"\")\n"
                }
            ], 
            "execution_count": 9
        }, 
        {
            "source": "pandas_df = sqlDF.toPandas()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 11
        }, 
        {
            "source": "pandas_df.to_sql(name = 'weather-observation-temperatures', con=engine, if_exists='replace', index=False, chunksize=1000)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 12
        }, 
        {
            "source": "df = pd.read_sql_table('weather-observation-temperatures', engine)\ndf.head(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "               obs_name    obs_date day_ind  mintemp  avgtemp  maxtemp\n0  Hamburg/Finkenwerder  2019-04-28       D        8      8.9       13\n1  Hamburg/Finkenwerder  2019-04-28       N        8      9.0       10\n2  Hamburg/Finkenwerder  2019-04-29       D        8     11.7       17\n3  Hamburg/Finkenwerder  2019-04-29       N        4      8.0       13\n4  Hamburg/Finkenwerder  2019-04-30       D        4     11.9       17", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obs_name</th>\n      <th>obs_date</th>\n      <th>day_ind</th>\n      <th>mintemp</th>\n      <th>avgtemp</th>\n      <th>maxtemp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hamburg/Finkenwerder</td>\n      <td>2019-04-28</td>\n      <td>D</td>\n      <td>8</td>\n      <td>8.9</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hamburg/Finkenwerder</td>\n      <td>2019-04-28</td>\n      <td>N</td>\n      <td>8</td>\n      <td>9.0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hamburg/Finkenwerder</td>\n      <td>2019-04-29</td>\n      <td>D</td>\n      <td>8</td>\n      <td>11.7</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hamburg/Finkenwerder</td>\n      <td>2019-04-29</td>\n      <td>N</td>\n      <td>4</td>\n      <td>8.0</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hamburg/Finkenwerder</td>\n      <td>2019-04-30</td>\n      <td>D</td>\n      <td>4</td>\n      <td>11.9</td>\n      <td>17</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 13, 
                    "metadata": {}
                }
            ], 
            "execution_count": 13
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}